#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ü–∞—Ä—Å–µ—Ä –∞—Ñ–∏—à–∏ –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä–æ–≤ –ü–µ—Ä–º–∏ —Å afisha.ru
–°–æ–∑–¥–∞—ë—Ç –∫–∞–ª–µ–Ω–¥–∞—Ä—å .ics —Å all-day —Å–æ–±—ã—Ç–∏—è–º–∏ –¥–ª—è –≤—Å–µ—Ö —Ñ–∏–ª—å–º–æ–≤ (–∫—Ä–æ–º–µ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö)
–° –ø–æ–ª–Ω–æ–π –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π, –¥–µ—Ç–∞–ª—è–º–∏ (–±–∞–Ω–Ω–µ—Ä, –æ–ø–∏—Å–∞–Ω–∏–µ, —Ä–µ–π—Ç–∏–Ω–≥) –∏ –±–µ–∑ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π
"""

import argparse
import logging
import os
import random
import re
import time
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup
from icalendar import Calendar, Event

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
BASE_URL = 'https://www.afisha.ru/prm/schedule_cinema/'
SCHEDULE_URL = BASE_URL  # –°—Ç—Ä–∞–Ω–∏—Ü–∞ 1
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
HEADERS = {
    'User-Agent': USER_AGENT,
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Sec-Fetch-Dest': 'document',
    'Sec-Fetch-Mode': 'navigate',
    'Sec-Fetch-Site': 'none',
    'Cache-Control': 'max-age=0',
    'Upgrade-Insecure-Requests': '1'
}

# –ó–∞–¥–µ—Ä–∂–∫–∏ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è HTTP 429
DELAYS = {
    'default': 5,    # –û–±—ã—á–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    'page': 8,       # –°—Ç—Ä–∞–Ω–∏—Ü—ã —Å–ø–∏—Å–∫–æ–≤
    'detail': 12,    # –î–µ—Ç–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ñ–∏–ª—å–º–æ–≤
    'retry': 10      # –ü–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏
}

def smart_delay(delay_type: str = 'default', multiplier: int = 1):
    """–£–º–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ —Å —Å–ª—É—á–∞–π–Ω–æ–π –≤–∞—Ä–∏–∞—Ü–∏–µ–π"""
    base_delay = DELAYS.get(delay_type, DELAYS['default'])
    delay = base_delay * multiplier + random.uniform(0, 3)
    time.sleep(delay)
    logger.debug(f"–ó–∞–¥–µ—Ä–∂–∫–∞ {delay_type}: {delay:.2f} —Å–µ–∫")

def make_request(session: requests.Session, url: str, delay_type: str = 'default') -> Optional[requests.Response]:
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç HTTP-–∑–∞–ø—Ä–æ—Å —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ retry"""
    smart_delay(delay_type)
    try:
        response = session.get(url, headers=HEADERS, timeout=30)
        if response.status_code == 429:
            logger.warning(f"HTTP 429 –¥–ª—è {url}. –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É.")
            time.sleep(60)  # –î–ª–∏–Ω–Ω–∞—è –ø–∞—É–∑–∞ –ø—Ä–∏ rate limit
            return make_request(session, url, 'retry')
        response.raise_for_status()
        return response
    except requests.RequestException as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ {url}: {e}")
        return None

def parse_date_from_timestamp(timestamp: str) -> Optional[datetime]:
    """–ü–∞—Ä—Å–∏—Ç timestamp –≤ datetime (–º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã)"""
    try:
        dt = datetime.fromtimestamp(int(timestamp) / 1000)
        return dt
    except ValueError:
        return None

def parse_schedule_calendar(soup: BeautifulSoup) -> Optional[datetime]:
    """–ü–∞—Ä—Å–∏—Ç –∫–∞–ª–µ–Ω–¥–∞—Ä—å –≤–∏–¥–∂–µ—Ç–∞ –¥–ª—è –±–ª–∏–∂–∞–π—à–µ–π –¥–∞—Ç—ã —Å–µ–∞–Ω—Å–∞"""
    calendar_div = soup.find('div', {'aria-label': '–ö–∞–ª–µ–Ω–¥–∞—Ä—å'})
    if not calendar_div:
        logger.warning("–ö–∞–ª–µ–Ω–¥–∞—Ä—å –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return None

    # –ò—â–µ–º –∞–∫—Ç–∏–≤–Ω—ã–µ –¥–∞—Ç—ã (–∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–µ <a>)
    active_days = calendar_div.find_all('a', class_=re.compile(r'pdT6c'))
    if not active_days:
        logger.warning("–ê–∫—Ç–∏–≤–Ω—ã–µ –¥–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return None

    # –ë–ª–∏–∂–∞–π—à–∞—è –¥–∞—Ç–∞ - –ø–µ—Ä–≤–∞—è –∞–∫—Ç–∏–≤–Ω–∞—è
    first_day = active_days[0]
    aria_label = first_day.get('aria-label', '')
    # –ü–∞—Ä—Å–∏–º –∏–∑ aria-label, –Ω–∞–ø—Ä. "8 –æ–∫—Ç—è–±—Ä—è"
    date_match = re.search(r'(\d+)\s+([–∞-—è]+)', aria_label.lower())
    if date_match:
        day = int(date_match.group(1))
        month_name = date_match.group(2)
        month_map = {
            '–æ–∫—Ç—è–±—Ä—è': 10, '–Ω–æ—è–±—Ä—è': 11, '–¥–µ–∫–∞–±—Ä—è': 12,
            '—è–Ω–≤–∞—Ä—è': 1, '—Ñ–µ–≤—Ä–∞–ª—è': 2, '–º–∞—Ä—Ç–∞': 3, '–∞–ø—Ä–µ–ª—è': 4, '–º–∞—è': 5,
            '–∏—é–Ω—è': 6, '–∏—é–ª—è': 7, '–∞–≤–≥—É—Å—Ç–∞': 8, '—Å–µ–Ω—Ç—è–±—Ä—è': 9
        }
        if month_name in month_map:
            # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —Ç–µ–∫—É—â–∏–π –≥–æ–¥
            current_year = datetime.now().year
            try:
                dt = datetime(current_year, month_map[month_name], day)
                logger.info(f"–ë–ª–∏–∂–∞–π—à–∞—è –¥–∞—Ç–∞ —Å–µ–∞–Ω—Å–∞: {dt.strftime('%d.%m.%Y')}")
                return dt.date()
            except ValueError:
                pass

    # Fallback: –∑–∞–≤—Ç—Ä–∞
    tomorrow = (datetime.now() + timedelta(days=1)).date()
    logger.info(f"Fallback –¥–∞—Ç–∞: –∑–∞–≤—Ç—Ä–∞ {tomorrow}")
    return tomorrow

def extract_movie_detail(session: requests.Session, movie_url: str) -> Dict[str, Any]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–µ—Ç–∞–ª–∏ —Ñ–∏–ª—å–º–∞: –±–∞–Ω–Ω–µ—Ä, –æ–ø–∏—Å–∞–Ω–∏–µ, –≤–æ–∑—Ä–∞—Å—Ç, —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ"""
    response = make_request(session, movie_url, 'detail')
    if not response:
        return {}

    soup = BeautifulSoup(response.text, 'html.parser')
    details = {}

    # –ë–∞–Ω–Ω–µ—Ä (img src)
    banner_img = soup.find('img', class_=re.compile(r'(poster|banner|hero-image)')) or soup.find('img', alt=re.compile(r'.*—Ñ–∏–ª—å–º.*', re.I))
    if banner_img:
        banner_url = urljoin(BASE_URL, banner_img.get('src', ''))
        if banner_url:
            details['banner'] = banner_url
            logger.debug(f"–ë–∞–Ω–Ω–µ—Ä: {banner_url}")

    # –í–æ–∑—Ä–∞—Å—Ç (12+, 16+ –∏ —Ç.–¥.)
    age_elem = soup.find('span', class_=re.compile(r'age|rating')) or soup.find(string=re.compile(r'\d+\+'))
    if age_elem:
        age_text = re.search(r'(\d+)\+', str(age_elem))
        if age_text:
            details['age'] = age_text.group(1) + '+'
            logger.debug(f"–í–æ–∑—Ä–∞—Å—Ç: {details['age']}")

    # –û–ø–∏—Å–∞–Ω–∏–µ –ø–æ–¥ "–û —Ñ–∏–ª—å–º–µ"
    desc_section = soup.find('h2', string=re.compile(r'–æ —Ñ–∏–ª—å–º–µ', re.I))
    if desc_section:
        desc_elem = desc_section.find_next_sibling('div', class_=re.compile(r'description|about'))
        if desc_elem:
            description = desc_elem.get_text(strip=True)[:300]  # –û–±—Ä–µ–∑–∞–µ–º –¥–æ 300 —Å–∏–º–≤–æ–ª–æ–≤
            if len(description) > 200:
                description += '...'
            details['description'] = description
            logger.debug(f"–û–ø–∏—Å–∞–Ω–∏–µ: {description[:50]}...")

    # –†–∞—Å–ø–∏—Å–∞–Ω–∏–µ (–±–ª–∏–∂–∞–π—à–∞—è –¥–∞—Ç–∞)
    schedule_date = parse_schedule_calendar(soup)
    if schedule_date:
        details['date'] = schedule_date

    return details

def extract_movie_from_list(soup: BeautifulSoup) -> List[Dict[str, Any]]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ–∏–ª—å–º—ã —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–ø–∏—Å–∫–∞ (–Ω–∞–∑–≤–∞–Ω–∏–µ, URL, –±–∞–∑–æ–≤–∞—è –¥–∞—Ç–∞/–≤—Ä–µ–º—è)"""
    movies = []
    # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Ñ–∏–ª—å–º–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è (–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ–¥ –≤–æ–∑–º–æ–∂–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É)
    movie_elements = soup.find_all('div', class_=re.compile(r'movie|film|item|card'))
    for elem in movie_elements:
        title_elem = elem.find('a', class_=re.compile(r'title|name|h3')) or elem.find('h3')
        if title_elem:
            title = title_elem.get_text(strip=True)
            url = urljoin(BASE_URL, title_elem.get('href', ''))
            if title and url:
                movie = {
                    'title': title,
                    'url': url,
                    'date': None,  # –ë—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–æ –∏–∑ –¥–µ—Ç–∞–ª–µ–π
                    'time': None   # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è all-day
                }
                movies.append(movie)
                logger.debug(f"–ù–∞–π–¥–µ–Ω —Ñ–∏–ª—å–º: {title}")

    logger.info(f"–ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–∞–π–¥–µ–Ω–æ {len(movies)} —Ñ–∏–ª—å–º–æ–≤")
    return movies

def parse_all_schedule_pages(session: requests.Session, max_pages: Optional[int] = None) -> List[Dict[str, Any]]:
    """–ü–∞—Ä—Å–∏—Ç –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞–≥–∏–Ω–∞—Ü–∏–∏"""
    all_movies = []
    existing_titles = set()
    current_page = 1
    page_count = 0

    while True:
        if max_pages and page_count >= max_pages:
            logger.info(f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü: {max_pages}")
            break

        page_url = SCHEDULE_URL if current_page == 1 else f"{BASE_URL}page{current_page}/"
        logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {current_page}: {page_url}")

        response = make_request(session, page_url, 'page')
        if not response:
            logger.warning(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {current_page} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ (404?) - –∑–∞–≤–µ—Ä—à–∞–µ–º")
            break

        soup = BeautifulSoup(response.text, 'html.parser')
        page_movies = extract_movie_from_list(soup)

        if not page_movies:
            logger.info(f"–ü—É—Å—Ç–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {current_page} - –∑–∞–≤–µ—Ä—à–∞–µ–º")
            break

        for movie in page_movies:
            if movie['title'] not in existing_titles:
                existing_titles.add(movie['title'])
                all_movies.append(movie)
                logger.debug(f"–î–æ–±–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π —Ñ–∏–ª—å–º: {movie['title']}")

        page_count += 1
        current_page += 1

    logger.info(f"–ò—Ç–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ–∏–ª—å–º–æ–≤: {len(all_movies)} –Ω–∞ {page_count} —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö")
    return all_movies

def create_event(movie: Dict[str, Any], details: Dict[str, Any], exclude_country: str = '–†–æ—Å—Å–∏—è') -> Optional[Event]:
    """–°–æ–∑–¥–∞—ë—Ç all-day —Å–æ–±—ã—Ç–∏–µ –±–µ–∑ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π"""
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä–∞–Ω—É (fallback: –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –Ω–µ —Ä–æ—Å—Å–∏–π—Å–∫–∏–π, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ)
    if 'country' in details and exclude_country.lower() in details['country'].lower():
        logger.info(f"–ü—Ä–æ–ø—É—Å–∫ —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ —Ñ–∏–ª—å–º–∞: {movie['title']}")
        return None

    # –î–∞—Ç–∞: –∏–∑ –¥–µ—Ç–∞–ª–µ–π –∏–ª–∏ fallback –∑–∞–≤—Ç—Ä–∞
    event_date = details.get('date', (datetime.now() + timedelta(days=1)).date())
    event_date_str = event_date.strftime('%Y%m%d')

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å —ç–º–æ–¥–∑–∏
    summary = f"üé• {movie['title']}"

    # –û–ø–∏—Å–∞–Ω–∏–µ –≤ Markdown
    description_parts = []
    if 'age' in details:
        description_parts.append(f"üé≠ –†–µ–π—Ç–∏–Ω–≥: {details['age']}")
    if 'banner' in details:
        description_parts.append(f"\n[![–ë–∞–Ω–Ω–µ—Ä —Ñ–∏–ª—å–º–∞]]({details['banner']})")
    if 'description' in details:
        description_parts.append(f"\nüìú –û —Ñ–∏–ª—å–º–µ:\n{details['description']}")
    description_parts.append("\nüóìÔ∏è –°–æ–±—ã—Ç–∏–µ –Ω–∞ –≤–µ—Å—å –¥–µ–Ω—å: —Ñ–∏–ª—å–º –≤ –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä–∞—Ö –ü–µ—Ä–º–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∞–∫—Ç—É–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ afisha.ru.")
    description_parts.append(f"\nüìç –ò—Å—Ç–æ—á–Ω–∏–∫: {movie['url']}")

    description = '\n'.join(description_parts)

    # –°–æ–∑–¥–∞—ë–º —Å–æ–±—ã—Ç–∏–µ
    event = Event()
    event.add('uid', f"afisha-movie-{hash(movie['title'])}@maxytre.github.io")
    event.add('summary', summary)
    event.add('dtstart', datetime.strptime(event_date_str, '%Y%m%d').date())  # VALUE=DATE –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
    event.add('dtend', (datetime.strptime(event_date_str, '%Y%m%d').date() + timedelta(days=1)))  # –°–ª–µ–¥—É—é—â–∏–π –¥–µ–Ω—å
    event.add('description', description)
    event.add('location', '–ö–∏–Ω–æ—Ç–µ–∞—Ç—Ä—ã –ü–µ—Ä–º–∏')
    # –ë–µ–∑ VALARM - –Ω–µ—Ç —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π
    # –ë–µ–∑ –≤—Ä–µ–º–µ–Ω–∏ - all-day

    logger.info(f"–°–æ–∑–¥–∞–Ω–æ —Å–æ–±—ã—Ç–∏–µ: {summary} –Ω–∞ {event_date}")
    return event

def main():
    parser = argparse.ArgumentParser(description="–ü–∞—Ä—Å–µ—Ä –∞—Ñ–∏—à–∏ –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä–æ–≤ –ü–µ—Ä–º–∏")
    parser.add_argument('--exclude-country', default='–†–æ—Å—Å–∏—è', help='–ò—Å–∫–ª—é—á–∏—Ç—å —Ñ–∏–ª—å–º—ã —Å—Ç—Ä–∞–Ω—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –†–æ—Å—Å–∏—è)')
    parser.add_argument('--delay', type=int, default=5, help='–ë–∞–∑–æ–≤–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö')
    parser.add_argument('--skip-details', action='store_true', help='–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ (–±—ã—Å—Ç—Ä–µ–µ)')
    parser.add_argument('--max-movies', type=int, default=None, help='–ú–∞–∫—Å–∏–º—É–º —Ñ–∏–ª—å–º–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –±–µ–∑ –ª–∏–º–∏—Ç–∞)')
    parser.add_argument('--max-pages', type=int, default=None, help='–ú–∞–∫—Å–∏–º—É–º —Å—Ç—Ä–∞–Ω–∏—Ü (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –±–µ–∑ –ª–∏–º–∏—Ç–∞)')

    args = parser.parse_args()

    # –ì–ª–æ–±–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
    for key in DELAYS:
        DELAYS[key] *= (args.delay / 5.0)  # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –ø–æ --delay

    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ –∞—Ñ–∏—à–∏ –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä–æ–≤ –ü–µ—Ä–º–∏")
    logger.info(f"–†–µ–∂–∏–º: {'–ë—ã—Å—Ç—Ä—ã–π (–±–µ–∑ –¥–µ—Ç–∞–ª–µ–π)' if args.skip_details else '–ü–æ–ª–Ω—ã–π'}")
    if args.max_movies:
        logger.info(f"–õ–∏–º–∏—Ç —Ñ–∏–ª—å–º–æ–≤: {args.max_movies}")
    if args.max_pages:
        logger.info(f"–õ–∏–º–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü: {args.max_pages}")

    session = requests.Session()
    session.headers.update(HEADERS)

    # –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
    all_movies = parse_all_schedule_pages(session, args.max_pages)

    if not all_movies:
        logger.error("–ù–µ—Ç —Ñ–∏–ª—å–º–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        return

    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ —Ñ–∏–ª—å–º–∞–º
    if args.max_movies:
        all_movies = all_movies[:args.max_movies]
        logger.info(f"–û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –¥–æ {len(all_movies)} —Ñ–∏–ª—å–º–æ–≤")

    events = []
    processed = 0
    for i, movie in enumerate(all_movies, 1):
        details = {}
        if not args.skip_details:
            details = extract_movie_detail(session, movie['url'])

        event = create_event(movie, details, args.exclude_country)
        if event:
            events.append(event)

        processed += 1
        if processed % 10 == 0:
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed}/{len(all_movies)} —Ñ–∏–ª—å–º–æ–≤, —Å–æ–∑–¥–∞–Ω–æ {len(events)} —Å–æ–±—ã—Ç–∏–π")

        # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Ñ–∏–ª—å–º–∞–º–∏
        if not args.skip_details:
            smart_delay('detail', 0.5)  # –ú–µ–Ω—å—à–µ –º–µ–∂–¥—É —Ñ–∏–ª—å–º–∞–º–∏

    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞–ª–µ–Ω–¥–∞—Ä—è
    cal = Calendar()
    cal.add('prodid', '-//Afisha Movie Calendar//MaxYtre//RU')
    cal.add('version', '2.0')
    cal.add('calscale', 'GREGORIAN')
    cal.add('method', 'PUBLISH')

    for event in events:
        cal.add_component(event)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    with open('calendar.ics', 'wb') as f:
        f.write(cal.to_ical())

    logger.info(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ: {len(events)} —Å–æ–±—ã—Ç–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ calendar.ics")

if __name__ == '__main__':
    main()
